{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multisensory integration architecture for sign-language recognition\n",
    "## Project for Neuro-Inspired Systems Engineering course at TUM\n",
    "\n",
    "Authors of the project: Tatyana Klimenko and Cristina Gil\n",
    "\n",
    "This script was written by Cristina Gil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: modeling sign-language gestures\n",
    "\n",
    "The following code aims to model from a basic perspective sign-language gestures of american sign language.\n",
    "<img src=\"american_sign_language.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        index  little  middle  ring  thumb\n",
      "MCP_aa    0.0     0.0     0.0   0.0    NaN\n",
      "MCP_fe    0.0     0.0     0.0   0.0    0.0\n",
      "PIP       0.0     0.0     0.0   0.0    NaN\n",
      "TMC_aa    NaN     NaN     NaN   NaN    0.0\n",
      "TMC_fe    NaN     NaN     NaN   NaN    0.0\n"
     ]
    }
   ],
   "source": [
    "# Letters\n",
    "letters = {}\n",
    "\n",
    "# Fingers: Index, Middle, Ring, Little\n",
    "fingers = ['index','middle','ring','little']\n",
    "\n",
    "# Joints\n",
    "'''\n",
    "Angles for fingers:  \n",
    "    - MCP_fe: metacarpophalangeal flexion/extension \n",
    "    - MCP_aa: metacarpophalangeal abduction/adduction\n",
    "    - PIP: Proximal-interphalangeal \n",
    "\n",
    "Angles for thumb:\n",
    "    - TMC_fe: trapeziometacarpal flexion/extension\n",
    "    - TMC_aa: trapeziometacarpal abduction/adduction\n",
    "    - MCP_fe: metacarpophalangeal flexion/extension\n",
    "'''\n",
    "angles = {'MCP_fe','MCP_aa','PIP'}\n",
    "angles_thumb = {'TMC_fe','TMC_aa','MCP_fe'}\n",
    "\n",
    "\n",
    "# Initialize angles to 0 (Rest position of the hand)\n",
    "default_params = defaultdict(dict)\n",
    "\n",
    "for finger in fingers:\n",
    "    for angle in angles:\n",
    "        default_params[finger][angle] = 0\n",
    "# Thumb\n",
    "for angle in angles_thumb:\n",
    "    default_params['thumb'][angle]=0\n",
    "\n",
    "# Create a dataframe from default_parameters\n",
    "df = pd.DataFrame.from_dict(default_params)\n",
    "print(df)\n",
    "\n",
    "# Transform the dataframe to a row vector\n",
    "array = df.as_matrix().ravel()\n",
    "array = array[~np.isnan(array)]\n",
    "\n",
    "# Number of parameters\n",
    "n_params = len(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametrize the letters\n",
    "\n",
    "# Letter A\n",
    "A = df.copy()\n",
    "A.loc['MCP_fe':'PIP','index':'ring']=90\n",
    "A.loc['TMC_aa','thumb']=-45\n",
    "letters['A']=A\n",
    "\n",
    "# Letter B\n",
    "B = df.copy()\n",
    "B.loc['TMC_fe','thumb']=180\n",
    "letters['B']=B\n",
    "\n",
    "# Letter D\n",
    "D = df.copy()\n",
    "D.loc['MCP_fe','little':'thumb']=45\n",
    "D.loc['PIP','little':'ring']=60\n",
    "D.loc['TMC_fe','thumb'] = 90\n",
    "letters['D']=D\n",
    "\n",
    "# Letter E\n",
    "E = df.copy()\n",
    "E.loc['MCP_fe','index':'thumb']=90\n",
    "E.loc['PIP','index':'ring']=90\n",
    "E.loc['TMC_aa','thumb']=-90\n",
    "letters['E']=E\n",
    "\n",
    "# Letter F\n",
    "F = df.copy()\n",
    "F.loc['MCP_fe','index']=30\n",
    "F.loc['PIP','index']=90\n",
    "F.loc['TMC_fe','thumb']=45\n",
    "F.loc['MCP_fe','thumb']=90\n",
    "letters['F']=F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A         index  little  middle  ring  thumb\n",
      "MCP_aa    0.0     0.0     0.0   0.0    NaN\n",
      "MCP_fe   90.0    90.0    90.0  90.0    0.0\n",
      "PIP      90.0    90.0    90.0  90.0    NaN\n",
      "TMC_aa    NaN     NaN     NaN   NaN  -45.0\n",
      "TMC_fe    NaN     NaN     NaN   NaN    0.0\n",
      "B         index  little  middle  ring  thumb\n",
      "MCP_aa    0.0     0.0     0.0   0.0    NaN\n",
      "MCP_fe    0.0     0.0     0.0   0.0    0.0\n",
      "PIP       0.0     0.0     0.0   0.0    NaN\n",
      "TMC_aa    NaN     NaN     NaN   NaN    0.0\n",
      "TMC_fe    NaN     NaN     NaN   NaN  180.0\n",
      "D         index  little  middle  ring  thumb\n",
      "MCP_aa    0.0     0.0     0.0   0.0    NaN\n",
      "MCP_fe    0.0    45.0    45.0  45.0   45.0\n",
      "PIP       0.0    60.0    60.0  60.0    NaN\n",
      "TMC_aa    NaN     NaN     NaN   NaN    0.0\n",
      "TMC_fe    NaN     NaN     NaN   NaN   90.0\n",
      "E         index  little  middle  ring  thumb\n",
      "MCP_aa    0.0     0.0     0.0   0.0    NaN\n",
      "MCP_fe   90.0    90.0    90.0  90.0   90.0\n",
      "PIP      90.0    90.0    90.0  90.0    NaN\n",
      "TMC_aa    NaN     NaN     NaN   NaN  -90.0\n",
      "TMC_fe    NaN     NaN     NaN   NaN    0.0\n",
      "F         index  little  middle  ring  thumb\n",
      "MCP_aa    0.0     0.0     0.0   0.0    NaN\n",
      "MCP_fe   30.0     0.0     0.0   0.0   90.0\n",
      "PIP      90.0     0.0     0.0   0.0    NaN\n",
      "TMC_aa    NaN     NaN     NaN   NaN    0.0\n",
      "TMC_fe    NaN     NaN     NaN   NaN   45.0\n"
     ]
    }
   ],
   "source": [
    "# Print letters structure for checking\n",
    "for key, value in letters.items():\n",
    "    print(key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creatig samples\n",
    "def create_samples(letter,n_samples,variance):\n",
    "    # Convert to array and eliminate nan values\n",
    "    array = letter.as_matrix().ravel()\n",
    "    array = array[~np.isnan(array)]\n",
    "    \n",
    "    # Create samples and add gausian noise\n",
    "    data = np.tile(array, (n_samples,1))\n",
    "    noise = np.random.normal(0, variance, data.shape)\n",
    "    params = data+noise\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "n_samples = 1000\n",
    "variance = 5 #For noise: 5 degrees in all the angles  \n",
    "n_letters = len(letters)\n",
    "\n",
    "# Create the parameters structure to feed the neural network\n",
    "params = np.zeros((n_letters*n_samples,n_params))\n",
    "# labels = np.zeros(n_letters*n_samples)\n",
    "labels = np.zeros((n_letters*n_samples,n_letters))\n",
    "key_labels = {} # Dictionary to store the number assigned to each letter\n",
    "\n",
    "l=0\n",
    "for key, letter in letters.items():\n",
    "    \n",
    "    # Params\n",
    "    new_params = create_samples(letter,n_samples,variance)\n",
    "    params[(l*n_samples):(l*n_samples+n_samples),:] = new_params\n",
    "    \n",
    "    # Labels\n",
    "    labels[(l*n_samples):(l*n_samples+n_samples),l]=1\n",
    "    l+=1\n",
    "    \n",
    "    # Labels\n",
    "    # labels[((l-1)*n_samples):((l-1)*n_samples+n_samples)]=l\n",
    "    key_labels[key]=l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Setting the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4800\n",
      "Val size: 100\n",
      "Test size: 100\n"
     ]
    }
   ],
   "source": [
    "# Split the data in training set, validation set and test set\n",
    "# Input data stored in params, labels stored in labels\n",
    "\n",
    "# Create random list of intergers to mask the input data\n",
    "a = np.arange(4999)\n",
    "np.random.shuffle(a)\n",
    "\n",
    "\n",
    "# Test set\n",
    "mask_test = np.zeros(n_letters*n_samples, dtype=bool)\n",
    "trues_test = a[:100]\n",
    "mask_test[trues_test] = True\n",
    "\n",
    "test_data = params[mask_test,:]\n",
    "test_labels = labels[mask_test,:]\n",
    "\n",
    "# Validation set\n",
    "mask_val = np.zeros(n_letters*n_samples, dtype=bool)\n",
    "trues_val = a[100:200]\n",
    "mask_val[trues_val]=True\n",
    "\n",
    "val_data = params[mask_val,:]\n",
    "val_labels = labels[mask_val,:]\n",
    "\n",
    "# Train set\n",
    "mask_train = (~mask_test)*(~mask_val)\n",
    "train_data = params[mask_train,:]\n",
    "train_labels = labels[mask_train,:]\n",
    "\n",
    "\n",
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Val size: %i\" % len(val_data))\n",
    "print(\"Test size: %i\" % len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 5)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twoLayerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(twoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H)\n",
    "        self.linear2 = nn.Linear(H, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 15 100 5\n"
     ]
    }
   ],
   "source": [
    "# N is batch size; D_in is input dimension; # H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, train_data.shape[1], 100, n_letters\n",
    "# print(N, D_in, H, D_out)\n",
    "\n",
    "net = twoLayerNet(D_in, H, D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a loss function and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our loss function and an Optimizer\n",
    "loss_fn = nn.MSELoss(reduction='mean') # reduction = 'sum'\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 15]) torch.Size([64, 5])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 15]) torch.Size([64, 5])\n",
      "0 362.082275390625\n",
      "1 320497.59375\n",
      "2 0.40497004985809326\n",
      "3 0.4046461284160614\n",
      "4 0.4043225049972534\n",
      "5 0.40399906039237976\n",
      "6 0.40367597341537476\n",
      "7 0.4033530652523041\n",
      "8 0.4030304551124573\n",
      "9 0.40270811319351196\n",
      "10 0.40238600969314575\n",
      "11 0.40206414461135864\n",
      "12 0.401742547750473\n",
      "13 0.4014212191104889\n",
      "14 0.40110015869140625\n",
      "15 0.4007793664932251\n",
      "16 0.40045878291130066\n",
      "17 0.4001384973526001\n",
      "18 0.39981845021247864\n",
      "19 0.39949867129325867\n",
      "20 0.3991791009902954\n",
      "21 0.39885979890823364\n",
      "22 0.39854079484939575\n",
      "23 0.39822202920913696\n",
      "24 0.39790353178977966\n",
      "25 0.3975852429866791\n",
      "26 0.39726722240448\n",
      "27 0.39694955945014954\n",
      "28 0.39663201570510864\n",
      "29 0.396314799785614\n",
      "30 0.3959977924823761\n",
      "31 0.3956810534000397\n",
      "32 0.39536458253860474\n",
      "33 0.3950483202934265\n",
      "34 0.39473235607147217\n",
      "35 0.3944166302680969\n",
      "36 0.39410117268562317\n",
      "37 0.3937859833240509\n",
      "38 0.39347097277641296\n",
      "39 0.3931563198566437\n",
      "40 0.3928418457508087\n",
      "41 0.39252758026123047\n",
      "42 0.3922136425971985\n",
      "43 0.3918999135494232\n",
      "44 0.39158645272254944\n",
      "45 0.39127326011657715\n",
      "46 0.3909602761268616\n",
      "47 0.3906475901603699\n",
      "48 0.3903351426124573\n",
      "49 0.3900229334831238\n",
      "50 0.3897109925746918\n",
      "51 0.3893992602825165\n",
      "52 0.38908785581588745\n",
      "53 0.38877663016319275\n",
      "54 0.3884657025337219\n",
      "55 0.3881549537181854\n",
      "56 0.3878444731235504\n",
      "57 0.3875342607498169\n",
      "58 0.38722431659698486\n",
      "59 0.3869146406650543\n",
      "60 0.3866051435470581\n",
      "61 0.3862959146499634\n",
      "62 0.38598689436912537\n",
      "63 0.3856782019138336\n",
      "64 0.3853696882724762\n",
      "65 0.38506144285202026\n",
      "66 0.3847534954547882\n",
      "67 0.3844456970691681\n",
      "68 0.3841382563114166\n",
      "69 0.3838309943675995\n",
      "70 0.38352400064468384\n",
      "71 0.3832172155380249\n",
      "72 0.38291066884994507\n",
      "73 0.3826044201850891\n",
      "74 0.38229840993881226\n",
      "75 0.3819926381111145\n",
      "76 0.38168710470199585\n",
      "77 0.3813818097114563\n",
      "78 0.38107672333717346\n",
      "79 0.3807719349861145\n",
      "80 0.38046738505363464\n",
      "81 0.3801630735397339\n",
      "82 0.37985900044441223\n",
      "83 0.3795551359653473\n",
      "84 0.3792515993118286\n",
      "85 0.37894827127456665\n",
      "86 0.3786451816558838\n",
      "87 0.37834230065345764\n",
      "88 0.378039687871933\n",
      "89 0.3777373135089874\n",
      "90 0.37743520736694336\n",
      "91 0.3771333694458008\n",
      "92 0.37683165073394775\n",
      "93 0.376530259847641\n",
      "94 0.37622910737991333\n",
      "95 0.3759281635284424\n",
      "96 0.3756274878978729\n",
      "97 0.37532705068588257\n",
      "98 0.3750268220901489\n",
      "99 0.3747268617153168\n",
      "100 0.3744271993637085\n",
      "101 0.37412768602371216\n",
      "102 0.3738284707069397\n",
      "103 0.37352946400642395\n",
      "104 0.3732306957244873\n",
      "105 0.3729321360588074\n",
      "106 0.3726339042186737\n",
      "107 0.3723357915878296\n",
      "108 0.37203797698020935\n",
      "109 0.371740460395813\n",
      "110 0.37144309282302856\n",
      "111 0.371146023273468\n",
      "112 0.3708491623401642\n",
      "113 0.37055253982543945\n",
      "114 0.3702561557292938\n",
      "115 0.3699600100517273\n",
      "116 0.3696640729904175\n",
      "117 0.36936843395233154\n",
      "118 0.3690730035305023\n",
      "119 0.3687777817249298\n",
      "120 0.3684828579425812\n",
      "121 0.36818811297416687\n",
      "122 0.3678935766220093\n",
      "123 0.36759933829307556\n",
      "124 0.36730530858039856\n",
      "125 0.36701154708862305\n",
      "126 0.36671799421310425\n",
      "127 0.36642464995384216\n",
      "128 0.36613157391548157\n",
      "129 0.3658387362957001\n",
      "130 0.3655461072921753\n",
      "131 0.3652537763118744\n",
      "132 0.3649616241455078\n",
      "133 0.36466968059539795\n",
      "134 0.36437803506851196\n",
      "135 0.3640865385532379\n",
      "136 0.36379534006118774\n",
      "137 0.36350440979003906\n",
      "138 0.3632136583328247\n",
      "139 0.36292314529418945\n",
      "140 0.3626328408718109\n",
      "141 0.3623427748680115\n",
      "142 0.3620530366897583\n",
      "143 0.3617634177207947\n",
      "144 0.36147403717041016\n",
      "145 0.36118489503860474\n",
      "146 0.3608960211277008\n",
      "147 0.36060741543769836\n",
      "148 0.36031898856163025\n",
      "149 0.36003080010414124\n",
      "150 0.35974279046058655\n",
      "151 0.35945504903793335\n",
      "152 0.35916754603385925\n",
      "153 0.35888028144836426\n",
      "154 0.358593225479126\n",
      "155 0.3583064079284668\n",
      "156 0.3580198585987091\n",
      "157 0.35773345828056335\n",
      "158 0.3574473261833191\n",
      "159 0.3571614623069763\n",
      "160 0.3568757474422455\n",
      "161 0.35659030079841614\n",
      "162 0.3563050925731659\n",
      "163 0.35602009296417236\n",
      "164 0.3557353913784027\n",
      "165 0.3554508090019226\n",
      "166 0.3551665246486664\n",
      "167 0.35488244891166687\n",
      "168 0.35459861159324646\n",
      "169 0.35431501269340515\n",
      "170 0.35403159260749817\n",
      "171 0.3537484407424927\n",
      "172 0.3534654676914215\n",
      "173 0.3531827926635742\n",
      "174 0.35290029644966125\n",
      "175 0.3526179790496826\n",
      "176 0.35233598947525024\n",
      "177 0.3520541787147522\n",
      "178 0.3517725467681885\n",
      "179 0.351491242647171\n",
      "180 0.3512100875377655\n",
      "181 0.3509291410446167\n",
      "182 0.3506484925746918\n",
      "183 0.35036802291870117\n",
      "184 0.3500877618789673\n",
      "185 0.3498077690601349\n",
      "186 0.3495279848575592\n",
      "187 0.3492484390735626\n",
      "188 0.34896907210350037\n",
      "189 0.3486899435520172\n",
      "190 0.34841105341911316\n",
      "191 0.3481323719024658\n",
      "192 0.3478539288043976\n",
      "193 0.34757569432258606\n",
      "194 0.34729766845703125\n",
      "195 0.34701988101005554\n",
      "196 0.3467423617839813\n",
      "197 0.34646499156951904\n",
      "198 0.34618785977363586\n",
      "199 0.3459109961986542\n",
      "200 0.3456343114376068\n",
      "201 0.34535786509513855\n",
      "202 0.345081627368927\n",
      "203 0.34480559825897217\n",
      "204 0.3445298373699188\n",
      "205 0.3442542850971222\n",
      "206 0.3439789116382599\n",
      "207 0.3437037765979767\n",
      "208 0.34342890977859497\n",
      "209 0.3431541919708252\n",
      "210 0.3428797423839569\n",
      "211 0.34260550141334534\n",
      "212 0.3423314690589905\n",
      "213 0.3420576751232147\n",
      "214 0.3417840301990509\n",
      "215 0.3415106534957886\n",
      "216 0.34123751521110535\n",
      "217 0.3409646153450012\n",
      "218 0.3406918942928314\n",
      "219 0.3404194116592407\n",
      "220 0.34014707803726196\n",
      "221 0.3398750126361847\n",
      "222 0.3396031856536865\n",
      "223 0.3393315374851227\n",
      "224 0.33906012773513794\n",
      "225 0.3387889564037323\n",
      "226 0.338517963886261\n",
      "227 0.3382472097873688\n",
      "228 0.33797669410705566\n",
      "229 0.3377063572406769\n",
      "230 0.3374362587928772\n",
      "231 0.33716633915901184\n",
      "232 0.3368966281414032\n",
      "233 0.33662718534469604\n",
      "234 0.336357980966568\n",
      "235 0.33608895540237427\n",
      "236 0.3358200788497925\n",
      "237 0.3355514705181122\n",
      "238 0.3352831304073334\n",
      "239 0.3350149393081665\n",
      "240 0.33474695682525635\n",
      "241 0.3344792127609253\n",
      "242 0.33421170711517334\n",
      "243 0.3339444100856781\n",
      "244 0.3336773216724396\n",
      "245 0.3334104120731354\n",
      "246 0.3331437110900879\n",
      "247 0.3328772783279419\n",
      "248 0.3326110243797302\n",
      "249 0.33234497904777527\n",
      "250 0.332079142332077\n",
      "251 0.3318135142326355\n",
      "252 0.33154812455177307\n",
      "253 0.33128297328948975\n",
      "254 0.33101794123649597\n",
      "255 0.3307532072067261\n",
      "256 0.3304886519908905\n",
      "257 0.33022430539131165\n",
      "258 0.3299601972103119\n",
      "259 0.32969626784324646\n",
      "260 0.3294326066970825\n",
      "261 0.32916906476020813\n",
      "262 0.32890579104423523\n",
      "263 0.32864275574684143\n",
      "264 0.32837986946105957\n",
      "265 0.3281172215938568\n",
      "266 0.3278547525405884\n",
      "267 0.32759252190589905\n",
      "268 0.32733049988746643\n",
      "269 0.3270686864852905\n",
      "270 0.3268071413040161\n",
      "271 0.32654571533203125\n",
      "272 0.3262845277786255\n",
      "273 0.32602354884147644\n",
      "274 0.3257627785205841\n",
      "275 0.3255022168159485\n",
      "276 0.32524189352989197\n",
      "277 0.3249817192554474\n",
      "278 0.3247218132019043\n",
      "279 0.32446205615997314\n",
      "280 0.32420259714126587\n",
      "281 0.32394322752952576\n",
      "282 0.32368412613868713\n",
      "283 0.3234252333641052\n",
      "284 0.3231665790081024\n",
      "285 0.32290807366371155\n",
      "286 0.32264983654022217\n",
      "287 0.3223917484283447\n",
      "288 0.3221338987350464\n",
      "289 0.32187619805336\n",
      "290 0.3216187357902527\n",
      "291 0.3213615119457245\n",
      "292 0.3211044669151306\n",
      "293 0.32084763050079346\n",
      "294 0.3205910325050354\n",
      "295 0.32033461332321167\n",
      "296 0.32007840275764465\n",
      "297 0.31982237100601196\n",
      "298 0.3195665776729584\n",
      "299 0.3193109631538391\n",
      "300 0.31905555725097656\n",
      "301 0.3188003599643707\n",
      "302 0.3185453712940216\n",
      "303 0.3182905912399292\n",
      "304 0.3180360496044159\n",
      "305 0.3177816569805145\n",
      "306 0.3175274431705475\n",
      "307 0.3172735273838043\n",
      "308 0.3170197010040283\n",
      "309 0.3167661726474762\n",
      "310 0.316512793302536\n",
      "311 0.31625962257385254\n",
      "312 0.31600669026374817\n",
      "313 0.31575390696525574\n",
      "314 0.3155013918876648\n",
      "315 0.3152490556240082\n",
      "316 0.3149968683719635\n",
      "317 0.3147449195384979\n",
      "318 0.31449317932128906\n",
      "319 0.3142416179180145\n",
      "320 0.3139902949333191\n",
      "321 0.31373918056488037\n",
      "322 0.313488245010376\n",
      "323 0.3132374584674835\n",
      "324 0.31298691034317017\n",
      "325 0.3127365708351135\n",
      "326 0.312486469745636\n",
      "327 0.3122365474700928\n",
      "328 0.3119867742061615\n",
      "329 0.3117372393608093\n",
      "330 0.31148791313171387\n",
      "331 0.31123876571655273\n",
      "332 0.3109898269176483\n",
      "333 0.3107410669326782\n",
      "334 0.31049251556396484\n",
      "335 0.31024420261383057\n",
      "336 0.3099960386753082\n",
      "337 0.3097480833530426\n",
      "338 0.3095003664493561\n",
      "339 0.3092528283596039\n",
      "340 0.3090054392814636\n",
      "341 0.30875828862190247\n",
      "342 0.30851131677627563\n",
      "343 0.3082645535469055\n",
      "344 0.3080179989337921\n",
      "345 0.30777162313461304\n",
      "346 0.3075254559516907\n",
      "347 0.30727946758270264\n",
      "348 0.3070337176322937\n",
      "349 0.3067881464958191\n",
      "350 0.3065427839756012\n",
      "351 0.3062976002693176\n",
      "352 0.30605262517929077\n",
      "353 0.30580776929855347\n",
      "354 0.30556321144104004\n",
      "355 0.30531883239746094\n",
      "356 0.3050746023654938\n",
      "357 0.3048305809497833\n",
      "358 0.3045867383480072\n",
      "359 0.3043431341648102\n",
      "360 0.3040997087955475\n",
      "361 0.3038564622402191\n",
      "362 0.30361345410346985\n",
      "363 0.3033705949783325\n",
      "364 0.3031279444694519\n",
      "365 0.3028855323791504\n",
      "366 0.3026432693004608\n",
      "367 0.3024011552333832\n",
      "368 0.30215927958488464\n",
      "369 0.3019176423549652\n",
      "370 0.3016761243343353\n",
      "371 0.30143484473228455\n",
      "372 0.3011937439441681\n",
      "373 0.30095285177230835\n",
      "374 0.30071210861206055\n",
      "375 0.30047163367271423\n",
      "376 0.30023127794265747\n",
      "377 0.2999911606311798\n",
      "378 0.2997512221336365\n",
      "379 0.29951146245002747\n",
      "380 0.2992718517780304\n",
      "381 0.2990325391292572\n",
      "382 0.29879331588745117\n",
      "383 0.29855436086654663\n",
      "384 0.2983155846595764\n",
      "385 0.29807695746421814\n",
      "386 0.2978385388851166\n",
      "387 0.29760029911994934\n",
      "388 0.2973622679710388\n",
      "389 0.297124445438385\n",
      "390 0.2968868017196655\n",
      "391 0.296649307012558\n",
      "392 0.29641208052635193\n",
      "393 0.29617494344711304\n",
      "394 0.295938104391098\n",
      "395 0.29570135474205017\n",
      "396 0.2954648733139038\n",
      "397 0.2952285706996918\n",
      "398 0.2949923872947693\n",
      "399 0.2947564721107483\n",
      "400 0.29452067613601685\n",
      "401 0.2942851185798645\n",
      "402 0.2940497398376465\n",
      "403 0.2938145697116852\n",
      "404 0.2935795783996582\n",
      "405 0.29334476590156555\n",
      "406 0.29311010241508484\n",
      "407 0.29287564754486084\n",
      "408 0.29264140129089355\n",
      "409 0.2924073338508606\n",
      "410 0.29217347502708435\n",
      "411 0.29193976521492004\n",
      "412 0.29170626401901245\n",
      "413 0.2914729118347168\n",
      "414 0.29123979806900024\n",
      "415 0.2910068929195404\n",
      "416 0.2907741367816925\n",
      "417 0.29054152965545654\n",
      "418 0.2903091609477997\n",
      "419 0.29007694125175476\n",
      "420 0.28984493017196655\n",
      "421 0.28961312770843506\n",
      "422 0.2893814742565155\n",
      "423 0.2891499996185303\n",
      "424 0.28891870379447937\n",
      "425 0.28868764638900757\n",
      "426 0.2884567379951477\n",
      "427 0.28822603821754456\n",
      "428 0.28799548745155334\n",
      "429 0.28776511549949646\n",
      "430 0.2875349819660187\n",
      "431 0.28730496764183044\n",
      "432 0.2870751619338989\n",
      "433 0.2868455648422241\n",
      "434 0.28661611676216125\n",
      "435 0.2863869071006775\n",
      "436 0.2861578166484833\n",
      "437 0.28592896461486816\n",
      "438 0.2857002317905426\n",
      "439 0.28547173738479614\n",
      "440 0.2852433919906616\n",
      "441 0.2850152552127838\n",
      "442 0.28478726744651794\n",
      "443 0.2845595180988312\n",
      "444 0.28433188796043396\n",
      "445 0.28410446643829346\n",
      "446 0.28387728333473206\n",
      "447 0.2836501896381378\n",
      "448 0.2834233343601227\n",
      "449 0.28319665789604187\n",
      "450 0.282970130443573\n",
      "451 0.28274381160736084\n",
      "452 0.2825176417827606\n",
      "453 0.2822916805744171\n",
      "454 0.28206586837768555\n",
      "455 0.2818402647972107\n",
      "456 0.28161484003067017\n",
      "457 0.28138959407806396\n",
      "458 0.2811645269393921\n",
      "459 0.28093963861465454\n",
      "460 0.2807149589061737\n",
      "461 0.2804903984069824\n",
      "462 0.28026607632637024\n",
      "463 0.2800418734550476\n",
      "464 0.27981793880462646\n",
      "465 0.2795941233634949\n",
      "466 0.2793704867362976\n",
      "467 0.27914705872535706\n",
      "468 0.27892374992370605\n",
      "469 0.27870067954063416\n",
      "470 0.2784777581691742\n",
      "471 0.27825501561164856\n",
      "472 0.27803245186805725\n",
      "473 0.2778100371360779\n",
      "474 0.277587890625\n",
      "475 0.2773658037185669\n",
      "476 0.27714401483535767\n",
      "477 0.2769223153591156\n",
      "478 0.27670079469680786\n",
      "479 0.2764795124530792\n",
      "480 0.27625834941864014\n",
      "481 0.27603742480278015\n",
      "482 0.2758166193962097\n",
      "483 0.2755959928035736\n",
      "484 0.2753755450248718\n",
      "485 0.27515533566474915\n",
      "486 0.274935245513916\n",
      "487 0.2747153043746948\n",
      "488 0.27449560165405273\n",
      "489 0.27427607774734497\n",
      "490 0.27405670285224915\n",
      "491 0.27383747696876526\n",
      "492 0.2736184597015381\n",
      "493 0.27339959144592285\n",
      "494 0.27318090200424194\n",
      "495 0.27296239137649536\n",
      "496 0.2727440893650055\n",
      "497 0.2725259065628052\n",
      "498 0.27230796217918396\n",
      "499 0.2720901370048523\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "print(x.shape, y.shape)\n",
    "print(type(x))\n",
    "\n",
    "x = torch.from_numpy(np.float32(train_data[:N,:]))\n",
    "y = torch.from_numpy(np.float32(train_labels[:N,:]))\n",
    "print(type(x))\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward pass\n",
    "    # y_pred = net(torch.from_numpy(train_data))\n",
    "    y_pred = net(x)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    # loss = loss_fn(y_pred, torch.from_numpy(train_labels))\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
